{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82b8ae2bc0e946df84c0716935e1c070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b173dd548419402b835aa84e5b403192",
              "IPY_MODEL_b5760434b7604047821bcf0f6fc3d0c2",
              "IPY_MODEL_ef5d1a1716624261a1ace468e0edffc8"
            ],
            "layout": "IPY_MODEL_753e564934c146c79c7cfea5362882d7"
          }
        },
        "b173dd548419402b835aa84e5b403192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e559af4b1b124ef29cbe08fe74486a13",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3b24d3d2ad714347b0ebb780f7056345",
            "value": "Batches:â€‡100%"
          }
        },
        "b5760434b7604047821bcf0f6fc3d0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9acc48e456ab4066ad464201ed3be490",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fff1dfd490494bb98fc1f049c870f2da",
            "value": 1
          }
        },
        "ef5d1a1716624261a1ace468e0edffc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4bcad5f3394f8699d73f43ee430266",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_db285dd2b52242589713135bfdf665fa",
            "value": "â€‡1/1â€‡[00:00&lt;00:00,â€‡â€‡1.73it/s]"
          }
        },
        "753e564934c146c79c7cfea5362882d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e559af4b1b124ef29cbe08fe74486a13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b24d3d2ad714347b0ebb780f7056345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9acc48e456ab4066ad464201ed3be490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fff1dfd490494bb98fc1f049c870f2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b4bcad5f3394f8699d73f43ee430266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db285dd2b52242589713135bfdf665fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJkOyMdBakxK",
        "outputId": "61a7dd10-e9c8-4168-b83d-d527c6977d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… All dependencies installed successfully!\n",
            "âœ… API configured!\n"
          ]
        }
      ],
      "source": [
        "# SECTION 1: ENVIRONMENT SETUP\n",
        "# =============================================================================\n",
        "# Why: Install all required dependencies for our hybrid RAG system\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q chromadb google-generativeai sentence-transformers huggingface_hub transformers\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Vector Database\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Google Gemini\n",
        "import google.generativeai as genai\n",
        "\n",
        "# For Open-Source LLM (Bonus Points)\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "print(\"âœ… All dependencies installed successfully!\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 2: API CONFIGURATION\n",
        "# =============================================================================\n",
        "# IMPORTANT: Replace with your actual API keys\n",
        "\n",
        "# Google Gemini API Key (Get from: https://makersuite.google.com/app/apikey)\n",
        "GEMINI_API_KEY = \"AIzaSyBTsU52ki26Lac6nJ-Z6iNFB5CgTT1vYSk\"\n",
        "\n",
        "# HuggingFace Token (Optional, for open-source LLM bonus)\n",
        "# Get from: https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"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\"  # Optional but recommended\n",
        "\n",
        "# Configure Gemini\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "print(\"âœ… API configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: LOAD TEXT DOCUMENTS\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "def load_text(path):\n",
        "    \"\"\"Reads a text file and returns its content as a string.\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# Prepare documents with content and metadata\n",
        "documents = {\n",
        "    \"employee_handbook_v1.txt\": {\n",
        "        \"content\": load_text(\"/content/employee_handbook_v1.txt\"),\n",
        "        \"metadata\": {\n",
        "            \"document_tag\": \"[GENERAL POLICY]\",\n",
        "            \"doc_type\": \"handbook\",\n",
        "            \"target_audience\": [\"employee\", \"manager\", \"intern\"],\n",
        "            \"effective_date\": \"2023-01-01\",\n",
        "            \"priority\": 1\n",
        "        }\n",
        "    },\n",
        "    \"intern_onboarding_faq.txt\": {\n",
        "        \"content\": load_text(\"/content/intern_onboarding_faq.txt\"),\n",
        "        \"metadata\": {\n",
        "            \"document_tag\": \"[INTERN POLICY]\",\n",
        "            \"doc_type\": \"faq\",\n",
        "            \"target_audience\": [\"intern\"],\n",
        "            \"effective_date\": \"2024-03-01\",\n",
        "            \"priority\": 3\n",
        "        }\n",
        "    },\n",
        "    \"manager_updates_2024.txt\": {\n",
        "        \"content\": load_text(\"/content/manager_updates_2024.txt\"),\n",
        "        \"metadata\": {\n",
        "            \"document_tag\": \"[MANAGEMENT GUIDANCE]\",\n",
        "            \"doc_type\": \"update\",\n",
        "            \"target_audience\": [\"employee\", \"manager\"],\n",
        "            \"effective_date\": \"2024-01-15\",\n",
        "            \"priority\": 2\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Loaded\", len(documents), \"documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TVFkjBNGGoM",
        "outputId": "d45fdea3-2525-4c2c-b30b-e3f6c587ea37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: STAGE 1 - METADATA-ENHANCED INGESTION\n",
        "# =============================================================================\n",
        "\"\"\"\n",
        "CONCEPT: Why Metadata Matters\n",
        "------------------------------\n",
        "Problem: Pure semantic search treats \"remote work for interns\" and\n",
        "         \"remote work for employees\" as nearly identical queries.\n",
        "\n",
        "Solution: Add structured metadata that allows us to:\n",
        "1. Filter by target audience BEFORE semantic search\n",
        "2. Boost relevant documents using priority scores\n",
        "3. Handle time-based policy conflicts (newer = more authoritative)\n",
        "\n",
        "We also PREPEND document tags to the content itself. Why?\n",
        "- The embedding model will learn that \"[INTERN POLICY]\" is semantically\n",
        "  different from \"[GENERAL POLICY]\"\n",
        "- This improves retrieval without relying solely on metadata filtering\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Optional # Added import for List, Dict and Optional\n",
        "\n",
        "class ConflictAwareRAG:\n",
        "    def __init__(self, embedding_model_name: str = \"all-MiniLM-L6-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system with ChromaDB and embedding model.\n",
        "\n",
        "        Why this embedding model?\n",
        "        - all-MiniLM-L6-v2: Fast, lightweight (80MB), good for local deployment\n",
        "        - Alternative: \"all-mpnet-base-v2\" (more accurate but slower)\n",
        "        \"\"\"\n",
        "        print(\"ğŸ”§ Initializing Conflict-Aware RAG System...\")\n",
        "\n",
        "        # Initialize ChromaDB (local, persistent storage)\n",
        "        self.chroma_client = chromadb.Client(Settings(\n",
        "            anonymized_telemetry=False,\n",
        "            is_persistent=False  # Set to True to save to disk\n",
        "        ))\n",
        "\n",
        "        # Create or get collection\n",
        "        self.collection = self.chroma_client.get_or_create_collection(\n",
        "            name=\"nebula_gears_policies\",\n",
        "            metadata={\"description\": \"Company policy documents with conflict resolution\"}\n",
        "        )\n",
        "\n",
        "        # Initialize embedding model (runs locally, no API calls)\n",
        "        print(f\"ğŸ“¥ Loading embedding model: {embedding_model_name}...\")\n",
        "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
        "\n",
        "        # Initialize Gemini\n",
        "        self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "\n",
        "        # Initialize HuggingFace client for open-source LLM (bonus points)\n",
        "        try:\n",
        "            self.hf_client = InferenceClient(token=HF_TOKEN)\n",
        "            self.use_opensource_llm = True\n",
        "            print(\"âœ… Open-source LLM enabled (Bonus Points!)\")\n",
        "        except:\n",
        "            self.use_opensource_llm = False\n",
        "            print(\"âš ï¸ Open-source LLM not available (HF token missing)\")\n",
        "\n",
        "        print(\"âœ… RAG System initialized!\\n\")\n",
        "\n",
        "    def ingest_documents(self, documents: Dict[str, Dict]):\n",
        "        \"\"\"\n",
        "        Stage 1: Ingest documents with metadata enhancement.\n",
        "\n",
        "        Key Innovation: We create \"augmented content\" by prepending tags.\n",
        "        This helps the embedding model understand document hierarchy.\n",
        "        \"\"\"\n",
        "        print(\"ğŸ“š STAGE 1: METADATA-ENHANCED INGESTION\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        doc_ids = []\n",
        "        doc_contents = []\n",
        "        doc_metadatas = []\n",
        "\n",
        "        for filename, doc_data in documents.items():\n",
        "            # Extract data\n",
        "            original_content = doc_data[\"content\"]\n",
        "            metadata = doc_data[\"metadata\"]\n",
        "\n",
        "            # CRITICAL: Prepend document tag to content\n",
        "            # This improves semantic understanding during retrieval\n",
        "            augmented_content = f\"{metadata['document_tag']} {original_content}\"\n",
        "\n",
        "            print(f\"\\nğŸ“„ Processing: {filename}\")\n",
        "            print(f\"   Tag: {metadata['document_tag']}\")\n",
        "            print(f\"   Target Audience: {metadata['target_audience']}\")\n",
        "            print(f\"   Priority: {metadata['priority']}\")\n",
        "\n",
        "            # Prepare for ChromaDB\n",
        "            doc_ids.append(filename)\n",
        "            doc_contents.append(augmented_content)\n",
        "\n",
        "            # Store metadata as JSON strings (ChromaDB requirement)\n",
        "            metadata_for_db = {\n",
        "                \"filename\": filename,\n",
        "                \"doc_type\": metadata[\"doc_type\"],\n",
        "                \"target_audience\": json.dumps(metadata[\"target_audience\"]),\n",
        "                \"effective_date\": metadata[\"effective_date\"],\n",
        "                \"priority\": metadata[\"priority\"],\n",
        "                \"original_content\": original_content  # Store original for display\n",
        "            }\n",
        "            doc_metadatas.append(metadata_for_db)\n",
        "\n",
        "        # Generate embeddings using SentenceTransformer\n",
        "        print(\"\\nğŸ”¢ Generating embeddings...\")\n",
        "        embeddings = self.embedding_model.encode(doc_contents, show_progress_bar=True)\n",
        "\n",
        "        # Add to ChromaDB\n",
        "        self.collection.add(\n",
        "            ids=doc_ids,\n",
        "            documents=doc_contents,\n",
        "            metadatas=doc_metadatas,\n",
        "            embeddings=embeddings.tolist()\n",
        "        )\n",
        "\n",
        "        print(f\"\\nâœ… Successfully ingested {len(doc_ids)} documents into ChromaDB!\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    def extract_role_from_query(self, query: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Stage 2A: Extract user role from query using pattern matching.\n",
        "\n",
        "        Why not always use LLM?\n",
        "        - Pattern matching is fast and free\n",
        "        - LLM is expensive for simple tasks\n",
        "        - We use LLM only as fallback\n",
        "        \"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Define role patterns (in priority order - most specific first)\n",
        "        role_patterns = {\n",
        "            \"intern\": [r\"\\bintern\\b\", r\"internship\", r\"new hire\", r\"just joined\"],\n",
        "            \"manager\": [r\"\\bmanager\\b\", r\"managing\", r\"team lead\"],\n",
        "            \"employee\": [r\"\\bemployee\\b\", r\"full-time\", r\"staff\"]\n",
        "        }\n",
        "\n",
        "        # Check patterns\n",
        "        for role, patterns in role_patterns.items():\n",
        "            for pattern in patterns:\n",
        "                if re.search(pattern, query_lower):\n",
        "                    print(f\"ğŸ¯ Role Extracted: '{role}' (Pattern: {pattern})\")\n",
        "                    return role\n",
        "\n",
        "        print(\"âš ï¸ No role detected in query, will retrieve all documents\")\n",
        "        return None\n",
        "\n",
        "    def retrieve_documents(self, query: str, n_results: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Stage 2B: Role-aware retrieval using hybrid search.\n",
        "\n",
        "        Hybrid Search = Semantic Similarity + Metadata Filtering\n",
        "\n",
        "        Process:\n",
        "        1. Extract role from query\n",
        "        2. Perform semantic search in ChromaDB\n",
        "        3. Filter/boost results based on target_audience metadata\n",
        "        4. Return top N ranked by relevance\n",
        "        \"\"\"\n",
        "        print(f\"\\nğŸ” STAGE 2: ROLE-AWARE RETRIEVAL\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Query: '{query}'\")\n",
        "\n",
        "        # Extract role\n",
        "        user_role = self.extract_role_from_query(query)\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode([query])[0]\n",
        "\n",
        "        # Retrieve from ChromaDB\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding.tolist()],\n",
        "            n_results=n_results\n",
        "        )\n",
        "\n",
        "        # Process results with metadata filtering\n",
        "        retrieved_docs = []\n",
        "        print(\"\\nğŸ“Š Retrieved Documents (with role filtering):\")\n",
        "\n",
        "        for i in range(len(results['ids'][0])):\n",
        "            doc_id = results['ids'][0][i]\n",
        "            distance = results['distances'][0][i]\n",
        "            metadata = results['metadatas'][0][i]\n",
        "            content = results['documents'][0][i]\n",
        "\n",
        "            # Parse target_audience from JSON string\n",
        "            target_audience = json.loads(metadata['target_audience'])\n",
        "\n",
        "            # Calculate relevance score\n",
        "            relevance_score = 1 - distance  # Convert distance to similarity\n",
        "\n",
        "            # METADATA BOOST: If user role matches target audience, boost score\n",
        "            role_match_boost = 0.0\n",
        "            if user_role and user_role in target_audience:\n",
        "                role_match_boost = 0.3  # Significant boost for role match\n",
        "                relevance_score += role_match_boost\n",
        "\n",
        "            # Add priority boost (higher priority = more specific policy)\n",
        "            priority_boost = metadata['priority'] * 0.05\n",
        "            relevance_score += priority_boost\n",
        "\n",
        "            retrieved_docs.append({\n",
        "                \"doc_id\": doc_id,\n",
        "                \"content\": metadata['original_content'],  # Use original content\n",
        "                \"metadata\": metadata,\n",
        "                \"relevance_score\": relevance_score,\n",
        "                \"role_match\": user_role in target_audience if user_role else False\n",
        "            })\n",
        "\n",
        "            print(f\"\\n  [{i+1}] {doc_id}\")\n",
        "            print(f\"      Similarity: {1-distance:.3f}\")\n",
        "            print(f\"      Role Match: {'âœ…' if (user_role in target_audience if user_role else False) else 'âŒ'}\")\n",
        "            print(f\"      Final Score: {relevance_score:.3f}\")\n",
        "\n",
        "        # Re-rank by final relevance score\n",
        "        retrieved_docs.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "\n",
        "        print(\"\\nâœ… Retrieval complete!\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"user_role\": user_role,\n",
        "            \"documents\": retrieved_docs\n",
        "        }\n",
        "\n",
        "    def detect_conflicts_opensource(self, retrieved_docs: List[Dict]) -> Dict:\n",
        "        \"\"\"\n",
        "        Stage 3: Conflict detection using open-source LLM (BONUS POINTS).\n",
        "\n",
        "        Why use a separate LLM for this?\n",
        "        - Conflict detection is a simpler task than answer generation\n",
        "        - We can use a smaller, cheaper model (Phi-3, Mistral 7B)\n",
        "        - Saves tokens on expensive Gemini calls\n",
        "\n",
        "        Model Choice: Mistral-7B-Instruct or Phi-3-mini\n",
        "        - Fast inference on HuggingFace's free tier\n",
        "        - Good at reasoning tasks\n",
        "        - Can run in Colab with quantization\n",
        "        \"\"\"\n",
        "        print(\"ğŸ¤– STAGE 3: CONFLICT DETECTION (Open-Source LLM)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if not self.use_opensource_llm:\n",
        "            print(\"âš ï¸ Skipping open-source LLM (not configured)\")\n",
        "            print(\"   Proceeding directly to Gemini answer generation...\")\n",
        "            print(\"=\" * 60 + \"\\n\")\n",
        "            return {\"conflict_detected\": False, \"analysis\": \"Skipped\"}\n",
        "\n",
        "        # Prepare document summaries for LLM\n",
        "        doc_summaries = []\n",
        "        for i, doc in enumerate(retrieved_docs[:3]):  # Top 3 only to save tokens\n",
        "            doc_summaries.append(\n",
        "                f\"Document {i+1} ({doc['doc_id']}):\\n\"\n",
        "                f\"Target Audience: {json.loads(doc['metadata']['target_audience'])}\\n\"\n",
        "                f\"Content: {doc['content'][:200]}...\\n\"\n",
        "            )\n",
        "\n",
        "        # Conflict detection prompt\n",
        "        conflict_prompt = f\"\"\"You are a policy analyst. Analyze these company documents for conflicts.\\n\\n{chr(10).join(doc_summaries)}\\n\\nTask: Do these documents contradict each other on remote work policy?\\nAnswer in this format:\\nCONFLICT: [YES/NO]\\nREASON: [One sentence explanation]\\nMOST_SPECIFIC: [Document number that is most specific to the user's role]\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"ğŸ”„ Sending to Mistral-7B-Instruct...\")\n",
        "\n",
        "            # Use HuggingFace Inference API (free tier)\n",
        "            response = self.hf_client.text_generation(\n",
        "                conflict_prompt,\n",
        "                model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.1\n",
        "            )\n",
        "\n",
        "            print(f\"\\nğŸ“ Conflict Analysis:\\n{response}\\n\")\n",
        "\n",
        "            # Parse response\n",
        "            conflict_detected = \"CONFLICT: YES\" in response.upper()\n",
        "\n",
        "            print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "            return {\n",
        "                \"conflict_detected\": conflict_detected,\n",
        "                \"analysis\": response\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Open-source LLM error: {str(e)}\")\n",
        "            print(\"   Falling back to direct Gemini generation...\")\n",
        "            print(\"=\" * 60 + \"\\n\")\n",
        "            return {\"conflict_detected\": False, \"analysis\": f\"Error: {str(e)}\"}\n",
        "\n",
        "    def generate_answer_with_gemini(\n",
        "        self,\n",
        "        query: str,\n",
        "        retrieved_docs: List[Dict],\n",
        "        conflict_info: Dict\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Stage 4: Final answer generation with conflict resolution.\n",
        "\n",
        "        Key Innovation: Structured prompt that enforces:\n",
        "        1. Policy hierarchy (specific > general, recent > old)\n",
        "        2. Mandatory source citation\n",
        "        3. Conflict explanation when applicable\n",
        "\n",
        "        Prompt Engineering Techniques Used:\n",
        "        - Role assignment (\"You are an HR assistant...\")\n",
        "        - Few-shot examples (implicit through rules)\n",
        "        - Output format constraints\n",
        "        - Chain-of-thought reasoning (\"Explain your reasoning\")\n",
        "        \"\"\"\n",
        "        print(\"ğŸ¨ STAGE 4: ANSWER GENERATION WITH CITATION (Gemini Flash)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Prepare document context for Gemini\n",
        "        doc_context = \"\"\n",
        "        for i, doc in enumerate(retrieved_docs[:3], 1):  # Top 3 documents\n",
        "            target_audience = json.loads(doc['metadata']['target_audience'])\n",
        "            doc_context += f\"\"\"\n",
        "---\n",
        "Document {i}: {doc['doc_id']}\n",
        "Target Audience: {', '.join(target_audience)}\n",
        "Effective Date: {doc['metadata']['effective_date']}\n",
        "Priority Level: {doc['metadata']['priority']} (higher = more specific)\n",
        "\n",
        "Content:\n",
        "{doc['content']}\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "        # Construct the prompt with conflict resolution logic\n",
        "        prompt = f\"\"\"You are an expert HR assistant at NebulaGears. Your job is to answer employee questions about company policies with precision and clarity.\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "AVAILABLE POLICY DOCUMENTS:\n",
        "{doc_context}\n",
        "\n",
        "CONFLICT RESOLUTION RULES (CRITICAL):\n",
        "1. **Role-Specific Beats General**: If a policy targets a specific role (e.g., \"interns\"), it ALWAYS overrides general policies\n",
        "2. **Recent Beats Old**: Newer effective dates override older policies\n",
        "3. **Specific Beats Broad**: Higher priority documents contain more specific rules\n",
        "\n",
        "YOUR TASK:\n",
        "1. Identify which document(s) are most relevant to the user's role\n",
        "2. If documents conflict, explain WHY you chose one over the others\n",
        "3. Provide a clear, direct answer to the question\n",
        "4. MANDATORY: Cite your source using this exact format: [Source: filename.txt]\n",
        "\n",
        "{\"âš ï¸ CONFLICT DETECTED: The open-source LLM flagged policy conflicts. Pay special attention to document priority and target audience.\" if conflict_info.get('conflict_detected') else \"\"}\n",
        "\n",
        "ANSWER FORMAT:\n",
        "[Your clear answer here]\n",
        "\n",
        "[If there's a conflict, explain: \"While [Document A] states X, [Document B] specifically applies to [role] and states Y. Therefore...\"]\n",
        "\n",
        "[Source: filename.txt]\n",
        "\n",
        "Now answer the question:\"\"\"\n",
        "\n",
        "        print(\"ğŸ”„ Sending to Gemini Flash 2.5...\")\n",
        "\n",
        "        # Generate response\n",
        "        response = self.gemini_model.generate_content(prompt)\n",
        "        answer = response.text\n",
        "\n",
        "        print(\"\\nâœ… Answer generated!\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def query(self, user_query: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Main query pipeline - orchestrates all 4 stages.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ğŸš€ STARTING CONFLICT-AWARE RAG PIPELINE\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        # Stage 2: Retrieval\n",
        "        retrieval_results = self.retrieve_documents(user_query)\n",
        "\n",
        "        # Stage 3: Conflict Detection (Open-Source LLM - Bonus)\n",
        "        conflict_info = self.detect_conflicts_opensource(retrieval_results['documents'])\n",
        "\n",
        "        # Stage 4: Answer Generation\n",
        "        final_answer = self.generate_answer_with_gemini(\n",
        "            user_query,\n",
        "            retrieval_results['documents'],\n",
        "            conflict_info\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"query\": user_query,\n",
        "            \"user_role\": retrieval_results['user_role'],\n",
        "            \"retrieved_documents\": retrieval_results['documents'],\n",
        "            \"conflict_analysis\": conflict_info,\n",
        "            \"answer\": final_answer\n",
        "        }"
      ],
      "metadata": {
        "id": "br12IX4nqx9l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION 5: INITIALIZE AND TEST THE SYSTEM\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ¯ INITIALIZING SYSTEM\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create RAG system instance\n",
        "rag_system = ConflictAwareRAG(embedding_model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Ingest documents\n",
        "rag_system.ingest_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622,
          "referenced_widgets": [
            "82b8ae2bc0e946df84c0716935e1c070",
            "b173dd548419402b835aa84e5b403192",
            "b5760434b7604047821bcf0f6fc3d0c2",
            "ef5d1a1716624261a1ace468e0edffc8",
            "753e564934c146c79c7cfea5362882d7",
            "e559af4b1b124ef29cbe08fe74486a13",
            "3b24d3d2ad714347b0ebb780f7056345",
            "9acc48e456ab4066ad464201ed3be490",
            "fff1dfd490494bb98fc1f049c870f2da",
            "4b4bcad5f3394f8699d73f43ee430266",
            "db285dd2b52242589713135bfdf665fa"
          ]
        },
        "id": "yH7i3CKoq85t",
        "outputId": "6bb7ccd9-cfcb-45b8-bf81-a2fed1baef2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ¯ INITIALIZING SYSTEM\n",
            "================================================================================\n",
            "\n",
            "ğŸ”§ Initializing Conflict-Aware RAG System...\n",
            "ğŸ“¥ Loading embedding model: all-MiniLM-L6-v2...\n",
            "âœ… Open-source LLM enabled (Bonus Points!)\n",
            "âœ… RAG System initialized!\n",
            "\n",
            "ğŸ“š STAGE 1: METADATA-ENHANCED INGESTION\n",
            "============================================================\n",
            "\n",
            "ğŸ“„ Processing: employee_handbook_v1.txt\n",
            "   Tag: [GENERAL POLICY]\n",
            "   Target Audience: ['employee', 'manager', 'intern']\n",
            "   Priority: 1\n",
            "\n",
            "ğŸ“„ Processing: intern_onboarding_faq.txt\n",
            "   Tag: [INTERN POLICY]\n",
            "   Target Audience: ['intern']\n",
            "   Priority: 3\n",
            "\n",
            "ğŸ“„ Processing: manager_updates_2024.txt\n",
            "   Tag: [MANAGEMENT GUIDANCE]\n",
            "   Target Audience: ['employee', 'manager']\n",
            "   Priority: 2\n",
            "\n",
            "ğŸ”¢ Generating embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82b8ae2bc0e946df84c0716935e1c070"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Successfully ingested 3 documents into ChromaDB!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_query = \"I just joined as a new intern. Can I work from home?\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ§ª TESTING WITH CHALLENGE QUERY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nQuery: '{test_query}'\\n\")\n",
        "\n",
        "result = rag_system.query(test_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "PBcqf4aHrAzo",
        "outputId": "4b151a14-35a6-409d-da79-2dc0f21522f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ§ª TESTING WITH CHALLENGE QUERY\n",
            "================================================================================\n",
            "\n",
            "Query: 'I just joined as a new intern. Can I work from home?'\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ STARTING CONFLICT-AWARE RAG PIPELINE\n",
            "================================================================================\n",
            "\n",
            "\n",
            "ğŸ” STAGE 2: ROLE-AWARE RETRIEVAL\n",
            "============================================================\n",
            "Query: 'I just joined as a new intern. Can I work from home?'\n",
            "ğŸ¯ Role Extracted: 'intern' (Pattern: \\bintern\\b)\n",
            "\n",
            "ğŸ“Š Retrieved Documents (with role filtering):\n",
            "\n",
            "  [1] intern_onboarding_faq.txt\n",
            "      Similarity: 0.040\n",
            "      Role Match: âœ…\n",
            "      Final Score: 0.490\n",
            "\n",
            "  [2] employee_handbook_v1.txt\n",
            "      Similarity: -0.269\n",
            "      Role Match: âœ…\n",
            "      Final Score: 0.081\n",
            "\n",
            "  [3] manager_updates_2024.txt\n",
            "      Similarity: -0.515\n",
            "      Role Match: âŒ\n",
            "      Final Score: -0.415\n",
            "\n",
            "âœ… Retrieval complete!\n",
            "============================================================\n",
            "\n",
            "ğŸ¤– STAGE 3: CONFLICT DETECTION (Open-Source LLM)\n",
            "============================================================\n",
            "ğŸ”„ Sending to Mistral-7B-Instruct...\n",
            "âš ï¸ Open-source LLM error: Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational.\n",
            "   Falling back to direct Gemini generation...\n",
            "============================================================\n",
            "\n",
            "ğŸ¨ STAGE 4: ANSWER GENERATION WITH CITATION (Gemini Flash)\n",
            "============================================================\n",
            "ğŸ”„ Sending to Gemini Flash 2.5...\n",
            "\n",
            "âœ… Answer generated!\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“‹ FINAL RESULT\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"â“ Question: {result['query']}\")\n",
        "print(f\"ğŸ‘¤ Detected Role: {result['user_role']}\")\n",
        "print(f\"\\nğŸ’¬ Answer:\\n{result['answer']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š RETRIEVAL ANALYSIS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"Top 3 Retrieved Documents:\")\n",
        "for i, doc in enumerate(result['retrieved_documents'][:3], 1):\n",
        "    print(f\"\\n{i}. {doc['doc_id']}\")\n",
        "    print(f\"   Relevance Score: {doc['relevance_score']:.3f}\")\n",
        "    print(f\"   Role Match: {'âœ…' if doc['role_match'] else 'âŒ'}\")\n",
        "    print(f\"   Target Audience: {json.loads(doc['metadata']['target_audience'])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PIPELINE EXECUTION COMPLETE!\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvw4SwJCrGHK",
        "outputId": "868651d4-49b2-476e-da0d-92b71b3f4f79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ“‹ FINAL RESULT\n",
            "================================================================================\n",
            "\n",
            "â“ Question: I just joined as a new intern. Can I work from home?\n",
            "ğŸ‘¤ Detected Role: intern\n",
            "\n",
            "ğŸ’¬ Answer:\n",
            "No, as a new intern at NebulaGears, you are required to be in the office 5 days a week for the duration of your internship to maximize mentorship. Remote work is generally not permitted for interns.\n",
            "\n",
            "While the `employee_handbook_v1.txt` and the `manager_updates_2024.txt` discuss remote work options for general employees, both documents explicitly state that interns are subject to role-specific guidelines. The `intern_onboarding_faq.txt` is a role-specific document for interns, and it clearly states: \"No remote work is permitted for interns\" and that \"interns are required to be in the office 5 days a week.\" This intern-specific policy takes precedence over the general employee policies.\n",
            "\n",
            "[Source: intern_onboarding_faq.txt]\n",
            "\n",
            "================================================================================\n",
            "ğŸ“Š RETRIEVAL ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Top 3 Retrieved Documents:\n",
            "\n",
            "1. intern_onboarding_faq.txt\n",
            "   Relevance Score: 0.490\n",
            "   Role Match: âœ…\n",
            "   Target Audience: ['intern']\n",
            "\n",
            "2. employee_handbook_v1.txt\n",
            "   Relevance Score: 0.081\n",
            "   Role Match: âœ…\n",
            "   Target Audience: ['employee', 'manager', 'intern']\n",
            "\n",
            "3. manager_updates_2024.txt\n",
            "   Relevance Score: -0.415\n",
            "   Role Match: âŒ\n",
            "   Target Audience: ['employee', 'manager']\n",
            "\n",
            "================================================================================\n",
            "âœ… PIPELINE EXECUTION COMPLETE!\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ§ª ADDITIONAL TEST CASES\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "additional_queries = [\n",
        "    \"Can I work remotely 3 days a week as a full-time employee?\",\n",
        "    \"What's the remote work policy for managers?\",\n",
        "]\n",
        "\n",
        "for test_query in additional_queries:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Query: {test_query}\")\n",
        "    print('='*80)\n",
        "\n",
        "    result = rag_system.query(test_query)\n",
        "    print(f\"\\nğŸ’¬ Answer:\\n{result['answer']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2RdZjpvOrMyv",
        "outputId": "00cce752-4272-494d-8acb-e1624aa46bce"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ§ª ADDITIONAL TEST CASES\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Query: Can I work remotely 3 days a week as a full-time employee?\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ STARTING CONFLICT-AWARE RAG PIPELINE\n",
            "================================================================================\n",
            "\n",
            "\n",
            "ğŸ” STAGE 2: ROLE-AWARE RETRIEVAL\n",
            "============================================================\n",
            "Query: 'Can I work remotely 3 days a week as a full-time employee?'\n",
            "ğŸ¯ Role Extracted: 'employee' (Pattern: \\bemployee\\b)\n",
            "\n",
            "ğŸ“Š Retrieved Documents (with role filtering):\n",
            "\n",
            "  [1] manager_updates_2024.txt\n",
            "      Similarity: 0.326\n",
            "      Role Match: âœ…\n",
            "      Final Score: 0.726\n",
            "\n",
            "  [2] employee_handbook_v1.txt\n",
            "      Similarity: 0.098\n",
            "      Role Match: âœ…\n",
            "      Final Score: 0.448\n",
            "\n",
            "  [3] intern_onboarding_faq.txt\n",
            "      Similarity: -0.114\n",
            "      Role Match: âŒ\n",
            "      Final Score: 0.036\n",
            "\n",
            "âœ… Retrieval complete!\n",
            "============================================================\n",
            "\n",
            "ğŸ¤– STAGE 3: CONFLICT DETECTION (Open-Source LLM)\n",
            "============================================================\n",
            "ğŸ”„ Sending to Mistral-7B-Instruct...\n",
            "âš ï¸ Open-source LLM error: Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational.\n",
            "   Falling back to direct Gemini generation...\n",
            "============================================================\n",
            "\n",
            "ğŸ¨ STAGE 4: ANSWER GENERATION WITH CITATION (Gemini Flash)\n",
            "============================================================\n",
            "ğŸ”„ Sending to Gemini Flash 2.5...\n",
            "\n",
            "âœ… Answer generated!\n",
            "============================================================\n",
            "\n",
            "\n",
            "ğŸ’¬ Answer:\n",
            "Yes, as a full-time employee, you can work remotely 3 days a week. However, you must be in the HQ office on Tuesdays and Thursdays, and all remote days require manager approval.\n",
            "\n",
            "While the `employee_handbook_v1.txt` states that employees are eligible for 100% remote work with no prior approval, the `manager_updates_2024.txt` is newer (effective 2024-01-15 vs. 2023-01-01) and has a higher priority level (2 vs. 1), making it the more specific and current policy that overrides the older, broader statement.\n",
            "\n",
            "[Source: manager_updates_2024.txt]\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Query: What's the remote work policy for managers?\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ STARTING CONFLICT-AWARE RAG PIPELINE\n",
            "================================================================================\n",
            "\n",
            "\n",
            "ğŸ” STAGE 2: ROLE-AWARE RETRIEVAL\n",
            "============================================================\n",
            "Query: 'What's the remote work policy for managers?'\n",
            "âš ï¸ No role detected in query, will retrieve all documents\n",
            "\n",
            "ğŸ“Š Retrieved Documents (with role filtering):\n",
            "\n",
            "  [1] manager_updates_2024.txt\n",
            "      Similarity: 0.454\n",
            "      Role Match: âŒ\n",
            "      Final Score: 0.554\n",
            "\n",
            "  [2] employee_handbook_v1.txt\n",
            "      Similarity: 0.075\n",
            "      Role Match: âŒ\n",
            "      Final Score: 0.125\n",
            "\n",
            "  [3] intern_onboarding_faq.txt\n",
            "      Similarity: -0.054\n",
            "      Role Match: âŒ\n",
            "      Final Score: 0.096\n",
            "\n",
            "âœ… Retrieval complete!\n",
            "============================================================\n",
            "\n",
            "ğŸ¤– STAGE 3: CONFLICT DETECTION (Open-Source LLM)\n",
            "============================================================\n",
            "ğŸ”„ Sending to Mistral-7B-Instruct...\n",
            "âš ï¸ Open-source LLM error: Model mistralai/Mistral-7B-Instruct-v0.2 is not supported for task text-generation and provider featherless-ai. Supported task: conversational.\n",
            "   Falling back to direct Gemini generation...\n",
            "============================================================\n",
            "\n",
            "ğŸ¨ STAGE 4: ANSWER GENERATION WITH CITATION (Gemini Flash)\n",
            "============================================================\n",
            "ğŸ”„ Sending to Gemini Flash 2.5...\n",
            "\n",
            "âœ… Answer generated!\n",
            "============================================================\n",
            "\n",
            "\n",
            "ğŸ’¬ Answer:\n",
            "Remote work for managers is capped at 3 days per week. You are required to be in the HQ office on Tuesdays and Thursdays. All remote days must have prior manager approval.\n",
            "\n",
            "While `employee_handbook_v1.txt` states that all employees are eligible for 100% remote work without approval, `manager_updates_2024.txt` is a newer document (effective 2024-01-15 vs. 2023-01-01) and has a higher priority level (2 vs. 1), making it the more current and specific policy that overrides the general handbook statement.\n",
            "\n",
            "[Source: manager_updates_2024.txt]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SECTION 9: COST ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ’° COST ANALYSIS FOR SCALING\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"\"\"\n",
        "SCENARIO: 10,000 documents, 5,000 queries/day\n",
        "\n",
        "ASSUMPTIONS:\n",
        "- Average document length: 100 tokens\n",
        "- Average query length: 20 tokens\n",
        "- Retrieved context per query: 5 chunks Ã— 150 tokens = 750 tokens\n",
        "- Average response: 150 tokens\n",
        "\n",
        "BREAKDOWN:\n",
        "\n",
        "1. ONE-TIME EMBEDDING COST (Initial ingestion):\n",
        "   - Documents: 10,000 docs Ã— 100 tokens = 1,000,000 tokens\n",
        "   - Using Google Gemini embeddings (text-embedding-004):\n",
        "     * FREE up to 1,500 requests/minute\n",
        "   - Cost: $0 (within free tier)\n",
        "\n",
        "   Alternative (Sentence Transformers):\n",
        "   - Runs locally, NO API COST\n",
        "   - One-time compute: ~30 minutes on CPU\n",
        "\n",
        "2. DAILY OPERATIONAL COST (Gemini Flash 2.5):\n",
        "\n",
        "   Input Tokens per Query:\n",
        "   - Query: 20 tokens\n",
        "   - System prompt: ~200 tokens\n",
        "   - Retrieved context: 750 tokens\n",
        "   - Total input: 970 tokens\n",
        "\n",
        "   Output Tokens per Query:\n",
        "   - Answer: 150 tokens\n",
        "\n",
        "   Daily Token Usage:\n",
        "   - Input: 5,000 queries Ã— 970 tokens = 4,850,000 tokens\n",
        "   - Output: 5,000 queries Ã— 150 tokens = 750,000 tokens\n",
        "\n",
        "   Gemini Flash 2.5 Pricing (as of Jan 2025):\n",
        "   - Input: $0.075 per 1M tokens\n",
        "   - Output: $0.30 per 1M tokens\n",
        "\n",
        "   Daily Cost:\n",
        "   - Input: (4.85M tokens Ã— $0.075) / 1M = $0.36\n",
        "   - Output: (0.75M tokens Ã— $0.30) / 1M = $0.23\n",
        "   - TOTAL: $0.59/day\n",
        "\n",
        "   Monthly Cost: $0.59 Ã— 30 = $17.70/month\n",
        "\n",
        "3. OPEN-SOURCE LLM BONUS (Conflict Detection):\n",
        "   - Using HuggingFace Inference API: FREE (rate-limited)\n",
        "   - Self-hosted Mistral 7B on cloud GPU: ~$0.50/hour\n",
        "   - If running conflict detection 20% of time: ~$3.60/day ($108/month)\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N45YwXcc0NUp",
        "outputId": "b0d648b8-adf6-435a-c12c-892a78768b18"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ’° COST ANALYSIS FOR SCALING\n",
            "================================================================================\n",
            "\n",
            "\n",
            "SCENARIO: 10,000 documents, 5,000 queries/day\n",
            "\n",
            "ASSUMPTIONS:\n",
            "- Average document length: 100 tokens\n",
            "- Average query length: 20 tokens\n",
            "- Retrieved context per query: 5 chunks Ã— 150 tokens = 750 tokens\n",
            "- Average response: 150 tokens\n",
            "\n",
            "BREAKDOWN:\n",
            "\n",
            "1. ONE-TIME EMBEDDING COST (Initial ingestion):\n",
            "   - Documents: 10,000 docs Ã— 100 tokens = 1,000,000 tokens\n",
            "   - Using Google Gemini embeddings (text-embedding-004):\n",
            "     * FREE up to 1,500 requests/minute\n",
            "   - Cost: $0 (within free tier)\n",
            "   \n",
            "   Alternative (Sentence Transformers):\n",
            "   - Runs locally, NO API COST\n",
            "   - One-time compute: ~30 minutes on CPU\n",
            "   \n",
            "2. DAILY OPERATIONAL COST (Gemini Flash 2.5):\n",
            "   \n",
            "   Input Tokens per Query:\n",
            "   - Query: 20 tokens\n",
            "   - System prompt: ~200 tokens\n",
            "   - Retrieved context: 750 tokens\n",
            "   - Total input: 970 tokens\n",
            "   \n",
            "   Output Tokens per Query:\n",
            "   - Answer: 150 tokens\n",
            "   \n",
            "   Daily Token Usage:\n",
            "   - Input: 5,000 queries Ã— 970 tokens = 4,850,000 tokens\n",
            "   - Output: 5,000 queries Ã— 150 tokens = 750,000 tokens\n",
            "   \n",
            "   Gemini Flash 2.5 Pricing (as of Jan 2025):\n",
            "   - Input: $0.075 per 1M tokens\n",
            "   - Output: $0.30 per 1M tokens\n",
            "   \n",
            "   Daily Cost:\n",
            "   - Input: (4.85M tokens Ã— $0.075) / 1M = $0.36\n",
            "   - Output: (0.75M tokens Ã— $0.30) / 1M = $0.23\n",
            "   - TOTAL: $0.59/day\n",
            "   \n",
            "   Monthly Cost: $0.59 Ã— 30 = $17.70/month\n",
            "   \n",
            "3. OPEN-SOURCE LLM BONUS (Conflict Detection):\n",
            "   - Using HuggingFace Inference API: FREE (rate-limited)\n",
            "   - Self-hosted Mistral 7B on cloud GPU: ~$0.50/hour\n",
            "   - If running conflict detection 20% of time: ~$3.60/day ($108/month)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psY-pI8l3d35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}